# ğŸ“Š SO SÃNH Káº¾T QUáº¢: TRÆ¯á»šC VÃ€ SAU KHI FIX

**NgÃ y**: 2026-01-20  
**Training Run**: Version 3 (vá»›i full fixes)

---

## âœ… TÃ“M Táº®T CÃC FIX ÄÃƒ APPLY

1. âœ… TÄƒng `entropy_coef = 0.3` (tá»« 0.1)
2. âœ… Giáº£m `pool_size = 32` (tá»« 64)
3. âœ… TÄƒng `episodes_per_gen = 500` (tá»« 200)
4. âœ… **FORCED EXPLORATION** (Gen 1-3): Äáº£m báº£o má»—i program Ä‘Æ°á»£c dÃ¹ng Ã­t nháº¥t 1 láº§n
5. âœ… **LEARNING RATE DECAY**: LR giáº£m 0.95^gen má»—i generation

---

## ğŸ“Š SO SÃNH CHÃNH

### **1. PROGRAM USAGE DIVERSITY** 

| Generation | TRÆ¯á»šC FIX (64 programs) | SAU FIX (32 programs) | Cáº£i thiá»‡n |
|------------|-------------------------|----------------------|-----------|
| **Gen 1** | 26/64 (40.6%) | **32/32 (100%)** | âœ… +59.4% |
| **Gen 2** | 1/64 (1.6%) | **32/32 (100%)** | âœ… +98.4% |
| **Gen 3** | 2/64 (3.1%) | **32/32 (100%)** | âœ… +96.9% |
| **Gen 4** | 1/64 (1.6%) | 4/32 (12.5%) | âœ… +10.9% |
| **Gen 5** | 2/64 (3.1%) | 2/32 (6.2%) | âœ… +3.1% |
| **Gen 10** | 2/64 (3.1%) | 3/32 (9.4%) | âœ… +6.3% |

**ğŸ’¡ PhÃ¢n tÃ­ch**:
- **Gen 1-3**: FORCED EXPLORATION hoáº¡t Ä‘á»™ng hoÃ n háº£o! Táº¤T Cáº¢ 32 programs Ä‘á»u Ä‘Æ°á»£c dÃ¹ng
- **Gen 4+**: Sau khi táº¯t forced exploration, PPO váº«n collapse nhÆ°ng Ã­t nghiÃªm trá»ng hÆ¡n (4-9% vs 1.6-3.1%)

---

### **2. TOP PROGRAM CONCENTRATION**

| Generation | TRÆ¯á»šC FIX | SAU FIX | Cáº£i thiá»‡n |
|------------|-----------|---------|-----------|
| **Gen 1** | 77.2% | **50.8%** | âœ… -26.4% |
| **Gen 2** | 100.0% | **92.5%** | âœ… -7.5% |
| **Gen 3** | 52.2% | **73.1%** | âŒ +20.9% |
| **Gen 4** | 100.0% | **67.3%** | âœ… -32.7% |
| **Gen 5** | 99.8% | **84.6%** | âœ… -15.2% |
| **Gen 10** | 99.8% | **71.1%** | âœ… -28.7% |

**ğŸ’¡ PhÃ¢n tÃ­ch**:
- Concentration giáº£m Ä‘Ã¡ng ká»ƒ nhÆ°ng VáºªN cao (50-92%)
- Tá»‘t nháº¥t á»Ÿ Gen 1 (50.8%) nhá» forced exploration
- Gen 4+ váº«n cÃ³ concentration 67-85% - váº«n chÆ°a lÃ½ tÆ°á»Ÿng

---

### **3. GINI COEFFICIENT (Inequality Measure)**

| Generation | TRÆ¯á»šC FIX | SAU FIX | Cáº£i thiá»‡n |
|------------|-----------|---------|-----------|
| **Gen 1** | 0.933 | **0.886** | âœ… -5.0% |
| **Gen 2** | 0.984 | **0.933** | âœ… -5.2% |
| **Gen 3** | 0.969 | **0.917** | âœ… -5.4% |
| **Gen 4-10** | 0.984 | **0.934-0.959** | âœ… -2.5% to -5.0% |

**ğŸ’¡ PhÃ¢n tÃ­ch**:
- Gini giáº£m tá»« 0.98 xuá»‘ng 0.89-0.96 (tá»‘t hÆ¡n nhÆ°ng váº«n cao)
- 0 = hoÃ n toÃ n equal, 1 = extreme concentration
- Target: <0.7 (chÆ°a Ä‘áº¡t Ä‘Æ°á»£c)

---

### **4. MAKESPAN PERFORMANCE**

| Metric | V1 (Broken, 64 progs) | V2 (Pool=64, entropy=0.1) | V3 (Pool=32, full fixes) |
|--------|----------------------|---------------------------|---------------------------|
| Gen 1 Avg | 171.04 Â± 45.06 | 171.04 Â± 45.06 | **170.48 Â± 45.23** âœ… |
| Gen 5 Avg | 143.16 Â± 41.48 | 143.16 Â± 41.48 | **169.22 Â± 44.54** âŒ |
| Gen 10 Avg | 150.38 Â± 39.21 | 150.38 Â± 39.21 | **169.99 Â± 45.60** âŒ |
| **Best** | **143.16** (Gen 5) | **143.16** (Gen 5) | 169.22 (Gen 5) |

**âš ï¸ Váº¤N Äá»€**:
- Makespan KHÃ”NG cáº£i thiá»‡n, tháº­m chÃ­ Tá»† HÆ N!
- Gen 5-10: ~169 (hiá»‡n táº¡i) vs ~143-150 (trÆ°á»›c Ä‘Ã³)
- **CÃ³ thá»ƒ do**: Forced exploration lÃ m giÃ¡n Ä‘oáº¡n convergence

---

### **5. PPO LEARNING STABILITY**

#### **PolicyLoss Progression**

| Generation | TRÆ¯á»šC FIX | SAU FIX | ÄÃ¡nh giÃ¡ |
|------------|-----------|---------|-----------|
| Gen 1 | 0.1116 | **0.0560** | âœ… Giáº£m 50% |
| Gen 5 | 0.0062 | **0.1505** | âŒ TÄƒng 24x |
| Gen 10 | 1.0562 (spike!) | **0.5664** | âœ… Giáº£m 46% |

**ğŸ’¡ PhÃ¢n tÃ­ch**:
- Gen 1: PolicyLoss tháº¥p hÆ¡n (tá»‘t!)
- Gen 5: PolicyLoss cao hÆ¡n (do forced exploration gÃ¢y nhiá»…u?)
- Gen 10: Váº«n cÃ³ spike nhÆ°ng NHá» HÆ N (0.57 vs 1.06)

#### **ValueLoss Progression**

| Generation | TRÆ¯á»šC FIX | SAU FIX | Cáº£i thiá»‡n |
|------------|-----------|---------|-----------|
| Gen 1 | 19,587 | **11,896** | âœ… -39.3% |
| Gen 5 | 3,920 | **3,653** | âœ… -6.8% |
| Gen 10 | 3,239 | **3,609** | âŒ +11.4% |

**ğŸ’¡ PhÃ¢n tÃ­ch**:
- ValueLoss tá»‘t hÆ¡n á»Ÿ Gen 1 (-39%)
- Gen 5-10 tÆ°Æ¡ng Ä‘Æ°Æ¡ng

---

### **6. LEARNING RATE DECAY**

```
Gen 1:  LR = 1.00e-4 (100%)
Gen 2:  LR = 9.50e-5 (95%)
Gen 5:  LR = 8.15e-5 (81%)
Gen 10: LR = 6.30e-5 (63%)
```

**âœ… Hoáº¡t Ä‘á»™ng nhÆ° expected!**
- LR giáº£m dáº§n má»—i gen theo 0.95^t
- GiÃºp stable training á»Ÿ later generations

---

## ğŸ¯ ÄÃNH GIÃ Tá»ªNG FIX

### **Fix 1: Forced Exploration (Gen 1-3)**

| Metric | Káº¿t quáº£ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| Programs used (Gen 1-3) | 32/32 (100%) | âœ… HOÃ€N Háº¢O! |
| Diversity improvement | +96.9% vs trÆ°á»›c | âœ… Ráº¤T Tá»T! |
| Impact on performance | Makespan tÄƒng ~15% | âš ï¸ TRADE-OFF |

**Káº¿t luáº­n**: 
- âœ… Fix hoáº¡t Ä‘á»™ng XUáº¤T Sáº®C vá» máº·t exploration
- âš ï¸ NhÆ°ng lÃ m giáº£m performance (makespan tá»‡ hÆ¡n)
- CÃ³ thá»ƒ do: 3 gens Ä‘áº§u khÃ³ converge â†’ áº£nh hÆ°á»Ÿng toÃ n bá»™ training

**Äá» xuáº¥t**: Giáº£m forced exploration xuá»‘ng 1-2 gens thay vÃ¬ 3

---

### **Fix 2: Increase Entropy (0.1 â†’ 0.3)**

| Metric | Káº¿t quáº£ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| Top1 concentration (Gen 4+) | 67-85% | âœ… Giáº£m tá»« 99-100% |
| Programs used (Gen 4+) | 2-4/32 | âœ… TÄƒng tá»« 1-2/64 |
| PolicyLoss stability | 0.57 (vs 1.06) | âœ… á»”n Ä‘á»‹nh hÆ¡n |

**Káº¿t luáº­n**:
- âœ… Fix hoáº¡t Ä‘á»™ng Tá»T
- Giáº£m concentration, tÄƒng diversity
- NhÆ°ng chÆ°a Ä‘á»§ Ä‘á»ƒ phÃ¡ vá»¡ hoÃ n toÃ n policy collapse

**Äá» xuáº¥t**: CÃ³ thá»ƒ tÄƒng thÃªm lÃªn 0.4-0.5

---

### **Fix 3: Reduce Pool Size (64 â†’ 32)**

| Metric | Káº¿t quáº£ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| Exploration coverage | 100% (Gen 1-3) | âœ… Dá»… cover hÆ¡n |
| Programs used % | 6-12% (Gen 4+) | âœ… TÄƒng tá»« 1.6-3.1% |
| Makespan | Tá»‡ hÆ¡n ~15% | âŒ TRADE-OFF |

**Káº¿t luáº­n**:
- âœ… GiÃºp PPO dá»… explore hÆ¡n vá»›i action space nhá» hÆ¡n
- âŒ NhÆ°ng giáº£m diversity cá»§a gene pool â†’ performance kÃ©m hÆ¡n
- âš ï¸ CÃ³ thá»ƒ 32 programs KHÃ”NG Äá»¦ Ä‘á»ƒ tÃ¬m ra good solutions

**Äá» xuáº¥t**: 
- **Option A**: TÄƒng láº¡i lÃªn 48 programs (compromise)
- **Option B**: Giá»¯ 32 nhÆ°ng train NHIá»€U HÆ N (20 gens thay vÃ¬ 10)

---

### **Fix 4: Increase Episodes (200 â†’ 500)**

| Metric | Káº¿t quáº£ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| Fitness estimate quality | Tá»‘t hÆ¡n (std tháº¥p hÆ¡n) | âœ… |
| Coverage | 100% vá»›i forced expl | âœ… |
| Training time | 2.5x cháº­m hÆ¡n | âš ï¸ |

**Káº¿t luáº­n**:
- âœ… GiÃºp estimate fitness chÃ­nh xÃ¡c hÆ¡n
- âœ… Äá»§ data Ä‘á»ƒ force 32 programs Ä‘Æ°á»£c dÃ¹ng
- âš ï¸ Trade-off: Training cháº­m hÆ¡n

---

### **Fix 5: Learning Rate Decay**

| Metric | Káº¿t quáº£ | ÄÃ¡nh giÃ¡ |
|--------|---------|----------|
| PolicyLoss spike (Gen 10) | 0.57 (vs 1.06) | âœ… -46% |
| Training stability | á»”n Ä‘á»‹nh hÆ¡n | âœ… |
| Convergence speed | Cháº­m hÆ¡n 1 chÃºt | âš ï¸ Acceptable |

**Káº¿t luáº­n**:
- âœ… Fix hoáº¡t Ä‘á»™ng Tá»T!
- Giáº£m PolicyLoss spike
- KhÃ´ng áº£nh hÆ°á»Ÿng tiÃªu cá»±c Ä‘áº¿n performance

---

## ğŸ” Váº¤N Äá»€ PHÃT HIá»†N Má»šI

### **Problem: Performance Regression**

**Makespan comparison**:
```
TrÆ°á»›c fix (64 programs, entropy=0.1):
  Gen 5: 143.16 (BEST)
  
Sau fix (32 programs, entropy=0.3, forced):
  Gen 5: 169.22 (Tá»† HÆ N +18%)
```

**Root causes phÃ¡t hiá»‡n**:

1. **Pool size 32 quÃ¡ nhá»**:
   - Vá»›i chá»‰ 32 programs, gene pool nghÃ¨o hÆ¡n
   - KhÃ³ tÃ¬m ra optimal combinations
   - Solution: TÄƒng lÃªn 48-64

2. **Forced exploration giÃ¡n Ä‘oáº¡n learning**:
   - 3 gens Ä‘áº§u PPO há»c lá»™n xá»™n (forced actions)
   - KhÃ´ng build momentum tá»‘t tá»« Ä‘áº§u
   - Solution: Chá»‰ force 1-2 gens, hoáº·c force nháº¹ hÆ¡n

3. **Entropy quÃ¡ cao (0.3)**:
   - PPO explore quÃ¡ nhiá»u, exploit quÃ¡ Ã­t
   - KhÃ´ng converge Ä‘Æ°á»£c vá» good programs
   - Solution: Giáº£m xuá»‘ng 0.2 hoáº·c decay entropy theo gen

---

## ğŸ’¡ Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š

### **Nhá»¯ng gÃ¬ THÃ€NH CÃ”NG âœ…**

1. **Forced Exploration**: Hoáº¡t Ä‘á»™ng XUáº¤T Sáº®C cho diversity (100% programs used Gen 1-3)
2. **Learning Rate Decay**: Giáº£m PolicyLoss spike hiá»‡u quáº£ (-46%)
3. **Smaller Action Space**: GiÃºp PPO dá»… explore hÆ¡n (6-12% vs 1.6-3.1%)
4. **Higher Entropy**: Giáº£m concentration (67-85% vs 99-100%)

### **Nhá»¯ng gÃ¬ CHÆ¯A Äáº T âŒ**

1. **Performance Regression**: Makespan tá»‡ hÆ¡n 18% (169 vs 143)
2. **Still High Concentration**: 67-85% váº«n quÃ¡ cao (target: <50%)
3. **Limited Diversity (Gen 4+)**: Chá»‰ 2-4/32 programs used

---

## ğŸš€ HÆ¯á»šNG TIáº¾P THEO

### **Strategy A: Balanced Approach (KHUYáº¾N NGHá»Š)**

```python
# config.py
pool_size = 48  # TÄƒng tá»« 32 (compromise giá»¯a diversity vÃ  learnability)
entropy_coef = 0.2  # Giáº£m tá»« 0.3 (Ã­t explore hÆ¡n, nhiá»u exploit hÆ¡n)
episodes_per_gen = 400  # Giáº£m tá»« 500 (faster training)

# trainer
forced_exploration_gens = 2  # Giáº£m tá»« 3 (1-2 gens thÃ´i)
```

**Expected**:
- Makespan: 145-155 (tá»‘t hÆ¡n hiá»‡n táº¡i)
- Programs used: 8-15/48 (16-31%)
- Top1 concentration: 40-60%

---

### **Strategy B: Aggressive Exploration**

```python
# config.py - GIá»® NGUYÃŠN nhÆ°ng train lÃ¢u hÆ¡n
num_generations = 20  # TÄƒng tá»« 10

# Add epsilon-greedy
epsilon = 0.1  # 10% random action
```

**Expected**:
- Diversity tá»‘t hÆ¡n nhÆ°ng performance uncertain
- Training time: 2x

---

### **Strategy C: Revert + Keep LR Decay**

```python
# Revert láº¡i config tá»‘t nháº¥t trÆ°á»›c Ä‘Ã³
pool_size = 64
entropy_coef = 0.1
episodes_per_gen = 200

# CHá»ˆ GIá»® LR decay (fix hoáº¡t Ä‘á»™ng tá»‘t nháº¥t)
# Bá» forced exploration
```

**Expected**:
- Makespan: ~143-150 (nhÆ° trÆ°á»›c)
- Diversity: Váº«n collapse nhÆ°ng stable hÆ¡n
- Fastest convergence

---

## ğŸ“Š RECOMMENDATION

**TÃ´i khuyáº¿n nghá»‹ Strategy A** vÃ¬:
1. CÃ¢n báº±ng giá»¯a diversity vÃ  performance
2. Giá»¯ Ä‘Æ°á»£c nhá»¯ng fix tá»‘t (LR decay, moderate forced expl)
3. Fix váº¥n Ä‘á» performance regression báº±ng pool_size=48

Báº¡n muá»‘n thá»­ strategy nÃ o?

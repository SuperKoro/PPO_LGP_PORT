# GIáº¢I THÃCH CHI TIáº¾T PROJECT PPO + LGP - PHáº¦N 4: PPO & COEVOLUTION

## ğŸ“‹ Má»¤C Lá»¤C PHáº¦N NÃ€Y
1. PPO Agent lÃ  gÃ¬?
2. PPO Architecture  
3. Coevolution - PPO + LGP cÃ¹ng tiáº¿n hÃ³a
4. Training Loop chi tiáº¿t
5. Tá»•ng káº¿t toÃ n bá»™ pipeline

---

## 4.1. PPO AGENT LÃ€ GÃŒ?

### **Reinforcement Learning cÆ¡ báº£n:**

**Má»¥c tiÃªu:** Train má»™t "agent" (AI) há»c **chá»n action tá»‘t nháº¥t** cho má»—i state

```
State â†’ [AGENT] â†’ Action â†’ Reward
```

**VÃ­ dá»¥ game Mario:**
```
State: Mario Ä‘á»©ng trÆ°á»›c há»‘
  â†“
Agent quyáº¿t Ä‘á»‹nh: NHáº¢Y!
  â†“
Action: Jump
  â†“  
Reward: +10 (qua Ä‘Æ°á»£c há»‘)
```

**Trong project nÃ y:**
```
State: [current_time=120, num_jobs=35, avg_pt=8.5]
  â†“
PPO Agent quyáº¿t Ä‘á»‹nh: Chá»n Portfolio #23
  â†“
Action: Run Portfolio #23 (EDD | SA:60%, GA:30%, PSO:10%)
  â†“
Reward: -makespan (vd: -180)
```

---

### **PPO (Proximal Policy Optimization)**

**PPO = Má»™t thuáº­t toÃ¡n RL hiá»‡n Ä‘áº¡i, á»•n Ä‘á»‹nh, hiá»‡u quáº£**

**Táº¡i sao dÃ¹ng PPO?**
- âœ… Stable (á»•n Ä‘á»‹nh hÆ¡n vanilla policy gradient)
- âœ… Sample efficient (há»c nhanh)
- âœ… Dá»… implement
- âœ… Widely used (OpenAI, DeepMind dÃ¹ng)

---

## 4.2. PPO ARCHITECTURE

### **File:** `training/ppo_model.py`

```python
class PPOActorCritic(nn.Module):
    """
    Neural network cho PPO
    """
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        
        # Shared layers (Actor vÃ  Critic dÃ¹ng chung)
        self.fc1 = nn.Linear(obs_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        
        # Actor head (chá»n action)
        self.actor = nn.Linear(64, act_dim)
        
        # Critic head (Ä‘Ã¡nh giÃ¡ state)
        self.critic = nn.Linear(64, 1)
    
    def forward(self, state):
        # Forward pass
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        
        # Actor: logits cho má»—i action
        logits = self.actor(x)
        
        # Critic: value cá»§a state
        value = self.critic(x)
        
        return logits, value
```

**Giáº£i thÃ­ch kiáº¿n trÃºc:**

```
Input: State [3 dimensions]
   â†“
Layer 1: Linear(3 â†’ 64) + ReLU
   â†“
Layer 2: Linear(64 â†’ 64) + ReLU
   â†“         â†“
Actor      Critic
(64 â†’ 64)  (64 â†’ 1)
   â†“         â†“
Logits    Value
(action)  (state)
```

---

### **Actor vs Critic:**

| Component | Má»¥c Ä‘Ã­ch | Output |
|-----------|----------|--------|
| **Actor** | Chá»n action | Logits (64 numbers) â†’ probabilities |
| **Critic** | ÄÃ¡nh giÃ¡ state | Value (1 number) = expected return |

**VÃ­ dá»¥:**

```python
state = torch.tensor([120.0, 35.0, 8.5])  # [time, jobs, avg_pt]
logits, value = model(state)

# Actor output:
logits = [0.5, -0.3, 1.2, ..., 0.8]  # 64 values
probs = softmax(logits) = [0.02, 0.01, 0.05, ..., 0.03]
# â†’ Portfolio 2 cÃ³ probability 5% Ä‘Æ°á»£c chá»n

# Critic output:
value = -250.5  # Expected total reward tá»« state nÃ y
```

---

### **Select Action:**

```python
# File: training/ppo_model.py
def select_action(model, state):
    """
    Chá»n action tá»« state
    """
    state_t = torch.FloatTensor(state).unsqueeze(0)  # [1, 3]
    
    with torch.no_grad():
        logits, value = model(state_t)
    
    # Sample action tá»« categorical distribution
    dist = torch.distributions.Categorical(logits=logits)
    action = dist.sample()  # Random theo probability
    log_prob = dist.log_prob(action)
    
    return action.item(), log_prob, value
```

**Step-by-step:**

```
State: [120.0, 35.0, 8.5]
   â†“
Model forward
   â†“
Logits: [0.5, -0.3, 1.2, 0.8, ...]
   â†“
Softmax â†’ Probabilities: [0.02, 0.01, 0.05, 0.03, ...]
   â†“
Sample (random): Action = 23 (chá»n portfolio #23)
   â†“
Return: (action=23, log_prob=-3.2, value=-250.5)
```

---

## 4.3. COEVOLUTION - PPO + LGP CÃ™NG TIáº¾N HÃ“A

### **KhÃ¡i niá»‡m Coevolution:**

**Coevolution = 2 populations tiáº¿n hÃ³a cÃ¹ng nhau vÃ  áº£nh hÆ°á»Ÿng láº«n nhau**

```
PPO Population: 1 PPO model
  â†• (interact)
LGP Population: 64 LGP programs
```

**VÃ²ng láº·p Coevolution:**

```
Generation N:
  1. LGP programs â†’ sinh portfolios
  2. PPO train vá»›i portfolios nÃ y
  3. PPO reward â†’ LGP fitness
  4. Evolve LGP programs
  5. Update PPO model
  â†“
Generation N+1: Repeat
```

---

### **Táº¡i sao Coevolution?**

**So sÃ¡nh cÃ¡c approaches:**

| Approach | PPO | LGP | Káº¿t quáº£ |
|----------|-----|-----|---------|
| **Only PPO** | Learn | Fixed portfolios | PPO tá»‘t nhÆ°ng bá»‹ giá»›i háº¡n bá»Ÿi portfolios |
| **Only LGP** | Fixed policy | Evolve | LGP evolve nhÆ°ng khÃ´ng biáº¿t state nÃ o dÃ¹ng gÃ¬ |
| **Coevolution** | Learn | Evolve | âœ… **BEST**: PPO há»c chá»n, LGP evolve portfolios |

**VÃ­ dá»¥:**
```
Gen 1:
  LGP: Portfolios tá»‡
  PPO: Há»c chá»n portfolio Ã­t tá»‡ nháº¥t
  
Gen 5:
  LGP: Portfolios tá»‘t hÆ¡n (nhá» PPO reward)
  PPO: Há»c chá»n portfolio tá»‘t trong sá»‘ tá»‘t hÆ¡n
  
Gen 10:
  LGP: Portfolios ráº¥t tá»‘t
  PPO: Expert á»Ÿ viá»‡c chá»n Ä‘Ãºng portfolio cho Ä‘Ãºng state
```

---

## 4.4. TRAINING LOOP CHI TIáº¾T

### **File:** `training/lgp_coevolution_trainer.py`

### **Main Loop:**

```python
def train_with_coevolution_lgp(env, lgp_programs, model, optimizer, cfg):
    """
    Coevolution training
    """
    K = len(lgp_programs)  # 64 programs
    
    for gen in range(cfg.num_generations):  # 10 generations
        print(f"Generation {gen+1}/10")
        
        # ============================================
        # STEP 1: LGP PROGRAMS â†’ PORTFOLIOS
        # ============================================
        lgp_inputs = build_lgp_inputs_for_env(env)
        
        action_library = []
        for prog in lgp_programs:
            portfolio = prog.execute(lgp_inputs)
            action_library.append(portfolio)
        
        env.action_library = action_library  # Gáº¯n vÃ o env
        
        # ============================================
        # STEP 2: PPO TRAINING
        # ============================================
        usage = np.zeros(K)      # Äáº¿m bao nhiÃªu láº§n má»—i portfolio Ä‘Æ°á»£c dÃ¹ng
        sum_reward = np.zeros(K)  # Tá»•ng reward má»—i portfolio
        
        for ep in range(cfg.episodes_per_gen):  # 500 episodes
            state = env.reset()
            
            states, actions, log_probs, values, rewards, masks = [], [], [], [], [], []
            ep_return = 0.0
            
            # --- EPISODE LOOP ---
            for step in range(cfg.max_steps_per_episode):
                # PPO chá»n action
                action, log_prob, value = select_action(model, state)
                
                # Environment step
                next_state, reward, done, _ = env.step(action)
                
                # Save trajectory
                states.append(state)
                actions.append(action)
                log_probs.append(log_prob)
                values.append(value)
                rewards.append(reward)
                masks.append(0.0 if done else 1.0)
                
                # Track usage & reward
                usage[action] += 1
                sum_reward[action] += reward
                ep_return += reward
                
                state = next_state
                if done:
                    break
            
            # --- PPO UPDATE ---
            returns = compute_returns(rewards, masks, gamma=0.9)
            advantages = returns - values
            
            for _ in range(4):  # 4 PPO epochs
                policy_loss, value_loss = compute_ppo_loss(
                    states, actions, log_probs, returns, advantages
                )
                loss = policy_loss + 0.5 * value_loss
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        # ============================================
        # STEP 3: COMPUTE LGP FITNESS
        # ============================================
        avg_reward = np.zeros(K)
        for i in range(K):
            if usage[i] > 0:
                avg_reward[i] = sum_reward[i] / usage[i]
            else:
                avg_reward[i] = -1e9  # Penalty cho unused
        
        # ============================================
        # STEP 4: EVOLVE LGP PROGRAMS
        # ============================================
        # Selection
        elite_indices = np.argsort(avg_reward)[-16:]  # Top 16
        elite = [lgp_programs[i] for i in elite_indices]
        
        # Crossover + Mutation
        children = []
        for _ in range(4):  # 4 children
            p1, p2 = random.sample(elite, 2)
            child = linear_crossover(p1, p2)
            child = mutate_program(child)
            children.append(child)
        
        # Replace worst programs
        worst_indices = np.argsort(avg_reward)[:4]  # Bottom 4
        for idx, child in zip(worst_indices, children):
            lgp_programs[idx] = child
        
        print(f"Best fitness: {avg_reward.max():.2f}")
    
    return lgp_programs, action_library
```

---

### **Detailed Breakdown:**

#### **STEP 1: LGP â†’ Portfolios**

```python
lgp_inputs = {
    "num_jobs": 20.0,
    "avg_processing_time": 8.0,
    "avg_ops_per_job": 2.5
}

action_library = []
for prog in lgp_programs:  # 64 programs
    portfolio = prog.execute(lgp_inputs)
    action_library.append(portfolio)

# Káº¿t quáº£:
# action_library = [Portfolio0, Portfolio1, ..., Portfolio63]
```

---

#### **STEP 2: PPO Training**

**Episode loop:**

```python
# Episode 1:
state = [0.0, 50.0, 10.0]  # Initial state

Step 1:
  PPO chá»n: action=23 (Portfolio #23)
  Env.step(23) â†’ reward=-180
  Next state: [45.0, 35.0, 8.5]

Step 2:
  PPO chá»n: action=12 (Portfolio #12)  
  Env.step(12) â†’ reward=-210
  Next state: [78.0, 20.0, 7.2]

Step 3:
  PPO chá»n: action=45 (Portfolio #45)
  Env.step(45) â†’ reward=-120
  Done = True

Total return = -180 + -210 + -120 = -510
```

**PPO Update:**

```python
# Compute returns (discounted)
returns = [-510, -330, -120]  # Simplified

# Compute advantages
advantages = returns - values
advantages = [-510 - (-400), -330 - (-250), -120 - (-100)]
          = [-110, -80, -20]

# Policy loss
ratio = exp(new_log_prob - old_log_prob)
policy_loss = -min(ratio * advantages, clipped_ratio * advantages)

# Value loss
value_loss = (returns - values)^2

# Update
loss = policy_loss + 0.5 * value_loss
optimizer.step()
```

---

#### **STEP 3: LGP Fitness**

```python
# Sau 500 episodes:
usage = [5, 3, 0, 12, 8, ..., 7]  # Sá»‘ láº§n má»—i portfolio Ä‘Æ°á»£c dÃ¹ng
sum_reward = [-250, -180, 0, -480, -320, ..., -280]

# TÃ­nh average reward = fitness
avg_reward = sum_reward / usage
avg_reward = [-50, -60, -inf, -40, -40, ..., -40]
#                                â†‘
#                          Portfolio #3 tá»‘t nháº¥t!
```

---

#### **STEP 4: Evolve LGP**

```python
# Selection
elite_indices = [3, 4, 63, 23, ...]  # Top 16 theo fitness
elite = [prog3, prog4, prog63, prog23, ...]

# Crossover
child1 = crossover(prog3, prog23)
child2 = crossover(prog4, prog63)
...

# Mutation
child1 = mutate(child1)
...

# Replace worst
worst_indices = [2, 17, 45, 55]  # Bottom 4
lgp_programs[2] = child1
lgp_programs[17] = child2
...
```

---

## 4.5. Tá»”NG Káº¾T TOÃ€N Bá»˜ PIPELINE

### **Full Pipeline Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INITIALIZATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Create Environment (DynamicSchedulingEnv)           â”‚
â”‚ 2. Initialize PPO Model (3â†’64â†’64â†’[64,1])              â”‚
â”‚ 3. Generate 64 random LGP programs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GENERATION LOOP (10 times)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PHASE 1: LGP â†’ PORTFOLIOS                     â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ For each LGP program:                         â”‚    â”‚
â”‚  â”‚   inputs = build_lgp_inputs(env)              â”‚    â”‚
â”‚  â”‚   portfolio = program.execute(inputs)         â”‚    â”‚
â”‚  â”‚ â†’ Get 64 portfolios                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PHASE 2: PPO TRAINING (# episodes)          â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ For each episode:                             â”‚    â”‚
â”‚  â”‚   state = env.reset()                         â”‚    â”‚
â”‚  â”‚   For each step:                              â”‚    â”‚
â”‚  â”‚     action = PPO.select(state)                â”‚    â”‚
â”‚  â”‚     next_state, reward = env.step(action)     â”‚    â”‚
â”‚  â”‚     track usage[action] += 1                  â”‚    â”‚
â”‚  â”‚     track sum_reward[action] += reward        â”‚    â”‚
â”‚  â”‚   PPO.update()                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PHASE 3: COMPUTE FITNESS                      â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ For each LGP program:                         â”‚    â”‚
â”‚  â”‚   fitness[i] = sum_reward[i] / usage[i]       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PHASE 4: EVOLVE LGP                           â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ 1. Select elite (top 16)                      â”‚    â”‚
â”‚  â”‚ 2. Crossover â†’ children                       â”‚    â”‚
â”‚  â”‚ 3. Mutation                                   â”‚    â”‚
â”‚  â”‚ 4. Replace worst programs                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â†“                                â”‚
â”‚  Save: metrics, programs, model                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FINAL OUTPUT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Trained PPO Model                                     â”‚
â”‚ â€¢ Evolved LGP Programs                                  â”‚
â”‚ â€¢ Training Metrics                                      â”‚
â”‚ â€¢ Visualization Plots                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Key Insights:**

1. **LGP creates diversity:**
   - 64 different programs â†’ 64 different portfolios
   - PPO cÃ³ nhiá»u options Ä‘á»ƒ chá»n

2. **PPO provides selection pressure:**
   - PPO chá»n programs tá»‘t thÆ°á»ng xuyÃªn hÆ¡n
   - Programs tá»‘t cÃ³ fitness cao â†’ Ä‘Æ°á»£c giá»¯ láº¡i

3. **Coevolution creates specialization:**
   - PPO há»c: "State X â†’ dÃ¹ng Program Y"
   - LGP evolve: "Program Y tá»‘i Æ°u cho State X"

4. **Result:**
   - PPO lÃ  "expert selector"
   - LGP programs lÃ  "specialized tools"

---

**â­ï¸ PHáº¦N 5 sáº½ lÃ  Code Walkthrough vá»›i vÃ­ dá»¥ cá»¥ thá»ƒ tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i!**

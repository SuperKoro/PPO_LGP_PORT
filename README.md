# PPO + LGP Dynamic Job Shop Scheduling

> A focused implementation combining Proximal Policy Optimization (PPO) with Linear Genetic Programming (LGP) for dynamic job shop scheduling problems.

## üéØ Overview

This clean implementation tackles the **Dynamic Job Shop Scheduling Problem** where:
- Initial jobs need to be scheduled across multiple machines
- New jobs arrive dynamically during execution  
- The system must reschedule unfinished jobs when disruptions occur
- **Goal**: Minimize makespan (total completion time)

### Key Innovation

The PPO agent learns to select **LGP-generated portfolios** where each portfolio is generated by an LGP program that outputs:
- **1 Dispatching Rule (DR)** - for job ordering (e.g., EDD, SPT, CR)
- **3 Metaheuristics (MH)** - for optimization (e.g., SA, GA, PSO) with weighted time budgets

Example portfolio output from LGP: `EDD | SA:52%, GA:25%, PSO:23%`

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LGP Coevolution Training Loop            ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  Generation  1 ‚Üí 2 ‚Üí ... ‚Üí N                    ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ PPO Agent                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Observes state (time, #jobs, avg_pt)     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Selects 1 of N LGP programs              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Learns which program for which state     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                       ‚Üï                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ LGP Evolution                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Evolves population of LGP programs       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Each program generates portfolios        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Programs evolve via crossover/mutation   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Result**: PPO learns *when* to use which LGP program, while LGP evolves *better* programs over time.

## üìÅ Project Structure

```
PPO_LGP_Clean/
‚îú‚îÄ‚îÄ core/                   # LGP components
‚îÇ   ‚îú‚îÄ‚îÄ lgp_program.py
‚îÇ   ‚îú‚îÄ‚îÄ lgp_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ lgp_instructions.py
‚îÇ   ‚îî‚îÄ‚îÄ lgp_evolution.py
‚îú‚îÄ‚îÄ environment/            # Scheduling environment
‚îÇ   ‚îú‚îÄ‚îÄ scheduling_env.py   (to be created)
‚îÇ   ‚îî‚îÄ‚îÄ env_utils.py
‚îú‚îÄ‚îÄ registries/             # DR and MH registries
‚îÇ   ‚îú‚îÄ‚îÄ dispatching_registry.py
‚îÇ   ‚îú‚îÄ‚îÄ dispatching_rules.py
‚îÇ   ‚îú‚îÄ‚îÄ mh_registry.py
‚îÇ   ‚îî‚îÄ‚îÄ metaheuristics_impl.py
‚îú‚îÄ‚îÄ training/               # Training components
‚îÇ   ‚îú‚îÄ‚îÄ ppo_model.py        (to be created)
‚îÇ   ‚îú‚îÄ‚îÄ lgp_coevolution_trainer.py
‚îÇ   ‚îî‚îÄ‚îÄ training_utils.py   (to be created)
‚îú‚îÄ‚îÄ scripts/                # Executable scripts
‚îÇ   ‚îú‚îÄ‚îÄ train_lgp.py        (to be created)
‚îÇ   ‚îú‚îÄ‚îÄ test_lgp.py
‚îÇ   ‚îî‚îÄ‚îÄ inference.py
‚îú‚îÄ‚îÄ analysis/               # Visualization tools
‚îÇ   ‚îú‚îÄ‚îÄ visualize_metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ analyze_evolution.py
‚îÇ   ‚îî‚îÄ‚îÄ compare_programs.py (to be created)
‚îú‚îÄ‚îÄ data/                   # Job shop instances
‚îÇ   ‚îî‚îÄ‚îÄ Set*.json
‚îú‚îÄ‚îÄ results/                # Training results
‚îÇ   ‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îú‚îÄ‚îÄ programs/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ plots/
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Installation

```bash
# Navigate to PPO_LGP_Clean folder
cd PPO_LGP_Clean

# Install dependencies
pip install -r requirements.txt
```

### Requirements
- Python 3.8+
- numpy >= 1.21.0
- torch >= 1.10.0
- gym >= 0.21.0
- matplotlib >= 3.5.0

## üìä Usage

### Training

```bash
python scripts/train_lgp.py
```

**Key Parameters** (in `config.py`):
- `num_generations`: Number of coevolution cycles
- `episodes_per_gen`: Episodes per generation  
- `population_size`: Number of LGP programs
- `elite_size`: Top programs to keep

**Output**:
- `results/metrics/generation_X.json`: Metrics per generation
- `results/programs/generation_X.json`: LGP program states
- `results/models/trained_policy.pth`: Trained PPO model

### Visualization

```bash
python analysis/visualize_metrics.py
```

Displays:
- Average reward over generations
- Policy loss and value loss
- Makespan trends
- LGP program fitness evolution

### Testing

```bash
python scripts/test_lgp.py
```

Tests LGP programs on sample instances.

### Inference

```bash
python scripts/inference.py
```

Loads trained model and runs test episodes.

## üìà LGP Program Structure

Each LGP program consists of:
- **Instructions**: Operations that manipulate registers
- **Registers**: Store intermediate values
- **Output Mapping**: Maps register values to portfolio components

Example LGP program output:
```python
Portfolio {
    DR: "EDD"           # From register R0
    MH1: "SA", 52%      # From register R1, R4
    MH2: "GA", 25%      # From register R2, R5
    MH3: "PSO", 23%     # From register R3, R6
}
```

## üîß Registry Pattern

The project uses the **Registry Pattern** to manage dispatching rules and metaheuristics dynamically.

### Adding a New Dispatching Rule

```python
# In registries/dispatching_rules.py
from registries.dispatching_registry import register_dr

@register_dr("FIFO")  # Auto-registers on import
def dr_fifo(env, finished_events, unfinished_jobs, time_budget_s=0.0):
    # Implementation...
    pass
```

### Adding a New Metaheuristic

```python
# In registries/metaheuristics_impl.py
from registries.mh_registry import register_mh

@register_mh("ACO")  # Auto-registers on import
def mh_aco(env, finished_events, unfinished_jobs, time_budget_s=1.0):
    # Implementation...
    pass
```

## üéì Key Concepts

### PPO State & Action

**State (Observation)**:
```python
[current_time, num_unfinished_operations, avg_processing_time]
```

**Action Space**: Discrete(N) - choose one of N LGP programs

### LGP Evolution

- **Crossover**: Swap instruction segments between programs
- **Mutation**: Randomly modify instructions, registers, or constants
- **Selection**: Keep elite programs, replace worst performers
- **Fitness**: Based on scheduling performance (makespan, tardiness)

## üìä Analysis Tools

### visualize_metrics.py
Plots training metrics across generations:
- Reward progression
- Loss curves  
- Makespan reduction
- Program fitness distribution

### analyze_evolution.py
Analyzes LGP evolution:
- Instruction frequency
- Register usage patterns
- Genetic diversity metrics

### compare_programs.py (to be created)
Compares different LGP programs:
- Head-to-head performance
- Portfolio composition analysis
- Instruction pattern comparison

## üî¨ Research Focus

This clean implementation is designed for:
1. **Deep understanding** of LGP-PPO coevolution
2. **Rapid experimentation** with different LGP configurations
3. **Clear separation** between components for easy modification
4. **Educational purposes** with well-documented code

## üìù Key Differences from Main Project

- **No GA Portfolio Evolution**: Uses only LGP for portfolio generation
- **Cleaner Structure**: Organized into logical modules
- **Focused Scope**: PPO + LGP only, no experimental features
- **Better Documentation**: Extensive comments and explanations

## ü§ù Contributing

This is a research project focused on PPO+LGP integration. Contributions that improve clarity, performance, or add new LGP features are welcome.

## üìÑ License

MIT License - See LICENSE file for details

## üîó Related Work

This implementation is based on research in:
- Proximal Policy Optimization (PPO)
- Linear Genetic Programming (LGP)
- Dynamic Job Shop Scheduling
- Evolutionary Algorithms

---

**Note**: This is a clean, focused implementation for research and learning. For production use, consider additional features like distributed training, checkpointing, and hyperparameter tuning.

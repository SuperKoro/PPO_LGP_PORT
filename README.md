# PPO + LGP Dynamic Job Shop Scheduling

> A focused implementation combining Proximal Policy Optimization (PPO) with Linear Genetic Programming (LGP) for dynamic job shop scheduling problems.

## ğŸ¯ Overview

This clean implementation tackles the **Dynamic Job Shop Scheduling Problem** where:
- Initial jobs need to be scheduled across multiple machines
- New jobs arrive dynamically during execution  
- The system must reschedule unfinished jobs when disruptions occur
- **Goal**: Minimize makespan (total completion time)

### Key Innovation

The PPO agent learns to select **LGP-generated portfolios** where each portfolio is generated by an LGP program that outputs:
- **1 Dispatching Rule (DR)** - for job ordering (e.g., EDD, SPT, CR)
- **3 Metaheuristics (MH)** - for optimization (e.g., SA, GA, PSO) with weighted time budgets

Example portfolio output from LGP: `EDD | SA:52%, GA:25%, PSO:23%`

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LGP Coevolution Training Loop            â”‚
â”‚                                                  â”‚
â”‚  Generation  1 â†’ 2 â†’ ... â†’ N                    â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PPO Agent                                   â”‚ â”‚
â”‚  â”‚ â€¢ Observes state (time, #jobs, avg_pt)     â”‚ â”‚
â”‚  â”‚ â€¢ Selects 1 of N LGP programs              â”‚ â”‚
â”‚  â”‚ â€¢ Learns which program for which state     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â†•                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LGP Evolution                               â”‚ â”‚
â”‚  â”‚ â€¢ Evolves population of LGP programs       â”‚ â”‚
â”‚  â”‚ â€¢ Each program generates portfolios        â”‚ â”‚
â”‚  â”‚ â€¢ Programs evolve via crossover/mutation   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result**: PPO learns *when* to use which LGP program, while LGP evolves *better* programs over time.

## ğŸ“ Project Structure

```
PPO_LGP_Clean/
â”œâ”€â”€ core/                   # LGP components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lgp_program.py      # LGP program representation
â”‚   â”œâ”€â”€ lgp_generator.py    # Generate LGP programs
â”‚   â”œâ”€â”€ lgp_instructions.py # LGP instruction set (8 types)
â”‚   â””â”€â”€ lgp_evolution.py    # Evolution operators
â”œâ”€â”€ environment/            # Scheduling environment
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scheduling_env.py   # Gym-style scheduling env
â”‚   â””â”€â”€ env_utils.py        # Utility functions
â”œâ”€â”€ registries/             # DR and MH registries
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dispatching_registry.py  # DR registry
â”‚   â”œâ”€â”€ dispatching_rules.py     # DR implementations (EDD, SPT, CR)
â”‚   â”œâ”€â”€ mh_registry.py           # MH registry
â”‚   â””â”€â”€ metaheuristics_impl.py   # MH implementations (SA, GA, PSO)
â”œâ”€â”€ training/               # Training components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ppo_model.py              # PPO actor-critic model
â”‚   â”œâ”€â”€ lgp_coevolution_trainer.py # Coevolution training loop
â”‚   â”œâ”€â”€ typed_action_adapter.py   # Action adapter
â”‚   â””â”€â”€ portfolio_types.py        # Portfolio data structures
â”œâ”€â”€ scripts/                # Executable scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_lgp.py        # Main training script
â”‚   â”œâ”€â”€ test_lgp.py         # Testing script
â”‚   â””â”€â”€ inference.py        # Inference script
â”œâ”€â”€ analysis/               # Visualization tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ visualize_metrics.py # Plot training metrics
â”‚   â””â”€â”€ analyze_evolution.py # Analyze LGP evolution
â”œâ”€â”€ data/                   # Job shop instances
â”‚   â””â”€â”€ Set*.json           # Training datasets
â”œâ”€â”€ results/                # Training outputs
â”‚   â”œâ”€â”€ metrics/            # JSON metrics per generation
â”‚   â”œâ”€â”€ programs/           # Saved LGP programs
â”‚   â”œâ”€â”€ models/             # Trained PPO models
â”‚   â””â”€â”€ plots/              # Generated visualizations
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ config.py               # Central configuration
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸš€ Installation

```bash
# Navigate to PPO_LGP_Clean folder
cd PPO_LGP_Clean

# Install dependencies
pip install -r requirements.txt

# Setup environment variables (if using API keys)
cp .env.example .env
# Edit .env and add your actual API keys
```

### Requirements
- Python 3.8+
- numpy >= 1.21.0
- torch >= 1.10.0
- gym >= 0.21.0
- matplotlib >= 3.5.0
- python-dotenv >= 0.19.0 (for environment variables)
- openpyxl >= 3.0.0 (for Excel support)

## ğŸ“Š Usage

### Training

```bash
python scripts/train_lgp.py
```

**Key Parameters** (in `config.py`):
- `num_generations`: Number of coevolution cycles
- `episodes_per_gen`: Episodes per generation  
- `population_size`: Number of LGP programs
- `elite_size`: Top programs to keep

**Output**:
- `results/metrics/generation_X.json`: Metrics per generation
- `results/programs/generation_X.json`: LGP program states
- `results/models/trained_policy.pth`: Trained PPO model

### Visualization

```bash
python analysis/visualize_metrics.py
```

Displays:
- Average reward over generations
- Policy loss and value loss
- Makespan trends
- LGP program fitness evolution

### Testing

```bash
python scripts/test_lgp.py
```

Tests LGP programs on sample instances.

### Inference

```bash
python scripts/inference.py
```

Loads trained model and runs test episodes.

## ğŸ”’ Security Best Practices

### API Keys & Secrets

This project uses environment variables to protect sensitive information like API keys.

**Never commit `.env` to Git!** It's already in `.gitignore`.

**Setup**:
1. Copy `.env.example` to `.env`:
   ```bash
   cp .env.example .env
   ```

2. Edit `.env` and add your actual keys:
   ```
   API_KEY=your_actual_api_key_here
   ```

3. Use in Python code:
   ```python
   from dotenv import load_dotenv
   import os
   
   load_dotenv()
   api_key = os.getenv("API_KEY")
   ```

> âš ï¸ **Important**: If you accidentally committed an API key, **revoke it immediately** and generate a new one. Git history preserves all changes!

---

## ğŸ“ˆ LGP Program Structure

Each LGP program consists of:
- **Instructions**: Operations that manipulate registers
- **Registers**: Store intermediate values
- **Output Mapping**: Maps register values to portfolio components

Example LGP program output:
```python
Portfolio {
    DR: "EDD"           # From register R0
    MH1: "SA", 52%      # From register R1, R4
    MH2: "GA", 25%      # From register R2, R5
    MH3: "PSO", 23%     # From register R3, R6
}
```

## ğŸ”§ Registry Pattern

The project uses the **Registry Pattern** to manage dispatching rules and metaheuristics dynamically.

### Adding a New Dispatching Rule

```python
# In registries/dispatching_rules.py
from registries.dispatching_registry import register_dr

@register_dr("FIFO")  # Auto-registers on import
def dr_fifo(env, finished_events, unfinished_jobs, time_budget_s=0.0):
    # Implementation...
    pass
```

### Adding a New Metaheuristic

```python
# In registries/metaheuristics_impl.py
from registries.mh_registry import register_mh

@register_mh("ACO")  # Auto-registers on import
def mh_aco(env, finished_events, unfinished_jobs, time_budget_s=1.0):
    # Implementation...
    pass
```

## ğŸ“ Key Concepts

### PPO State & Action

**State (Observation)**:
```python
[current_time, num_unfinished_operations, avg_processing_time]
```

**Action Space**: Discrete(N) - choose one of N LGP programs

### LGP Evolution

- **Crossover**: Swap instruction segments between programs
- **Mutation**: Randomly modify instructions, registers, or constants
- **Selection**: Keep elite programs, replace worst performers
- **Fitness**: Based on scheduling performance (makespan, tardiness)

## ğŸ“Š Analysis Tools

### visualize_metrics.py
Plots training metrics across generations:
- Reward progression
- Loss curves  
- Makespan reduction
- Program fitness distribution

### analyze_evolution.py
Analyzes LGP evolution:
- Instruction frequency
- Register usage patterns
- Genetic diversity metrics



## ğŸ”¬ Research Focus

This clean implementation is designed for:
1. **Deep understanding** of LGP-PPO coevolution
2. **Rapid experimentation** with different LGP configurations
3. **Clear separation** between components for easy modification
4. **Educational purposes** with well-documented code

## ğŸ“ Key Differences from Main Project

- **No GA Portfolio Evolution**: Uses only LGP for portfolio generation
- **Cleaner Structure**: Organized into logical modules
- **Focused Scope**: PPO + LGP only, no experimental features
- **Better Documentation**: Extensive comments and explanations

## ğŸ¤ Contributing

This is a research project focused on PPO+LGP integration. Contributions that improve clarity, performance, or add new LGP features are welcome.

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ”— Related Work

This implementation is based on research in:
- Proximal Policy Optimization (PPO)
- Linear Genetic Programming (LGP)
- Dynamic Job Shop Scheduling
- Evolutionary Algorithms

---

**Note**: This is a clean, focused implementation for research and learning. For production use, consider additional features like distributed training, checkpointing, and hyperparameter tuning.

# ğŸ“Š PHÃ‚N TÃCH Táº I SAO MAKESPAN KHÃ”NG Há»˜I Tá»¤

## ğŸ“ˆ TÃŒNH TRáº NG HIá»†N Táº I

```
Gen | Makespan | Std    | Change
---------------------------------
  1 |   166.31 |  45.30 | ---
  5 |   174.57 |  44.47 | +8.26 âŒ
 10 |   164.39 |  40.51 | -10.18 âœ…
 15 |   157.72 |  43.01 | -6.67 âœ… (BEST!)
 20 |   158.08 |  43.71 | +0.36 â†’

Range: 157.72 - 174.57 (16.85 dao Ä‘á»™ng!)
Average Std: 43.98 (Ráº¤T CAO!)
```

**Observation**: Makespan dao Ä‘á»™ng lÃªn xuá»‘ng, KHÃ”NG cÃ³ xu hÆ°á»›ng giáº£m á»•n Ä‘á»‹nh!

---

## ğŸ” **6 NGUYÃŠN NHÃ‚N Gá»C Rá»„**

---

### **1ï¸âƒ£ ENVIRONMENT STOCHASTICITY (Chá»§ yáº¿u)**

**Báº±ng chá»©ng**:
```
Gen  1: min=78, max=305, range=227
Gen 10: min=104, max=374, range=270
Gen 20: min=81, max=330, range=250
```

**NguyÃªn nhÃ¢n**: Má»—i episode lÃ  má»™t bÃ i toÃ¡n KHÃC NHAU!

```python
# scheduling_env.py - Dynamic jobs are RANDOM:
- num_dynamic = 2-4 jobs/episode (random)
- 25% Urgent, 75% Normal (random)
- 1-5 operations per job (random)
- Processing time: 5-50 (random)
- Arrival time: Exponential distribution (random)
```

**Háº­u quáº£**:
- Episode cÃ³ Ã­t dynamic jobs â†’ makespan tháº¥p (78-100)
- Episode cÃ³ nhiá»u urgent jobs phá»©c táº¡p â†’ makespan cao (300-374)
- **VARIANCE Cá»°C CAO (std ~44) LÃ€ UNAVOIDABLE!**

**ğŸ’¡ Insight**: ÄÃ¢y KHÃ”NG pháº£i bug - lÃ  Ä‘áº·c tÃ­nh cá»§a bÃ i toÃ¡n DYNAMIC scheduling!

---

### **2ï¸âƒ£ POLICY KHÃ”NG Há»ŒC ÄÆ¯á»¢C (NghiÃªm trá»ng)**

**Báº±ng chá»©ng - Within-generation learning**:
```
Gen  1: First 50 eps avg=-303.15, Last 50 eps avg=-309.56, Change=-6.41 âŒ
Gen  5: First 50 eps avg=-315.47, Last 50 eps avg=-325.84, Change=-10.37 âŒ
Gen 10: First 50 eps avg=-294.59, Last 50 eps avg=-309.06, Change=-14.47 âŒ
Gen 20: First 50 eps avg=-297.63, Last 50 eps avg=-310.44, Change=-12.81 âŒ
```

**Policy Ä‘ang Tá»† ÄI trong má»—i generation!** Return giáº£m tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i má»—i gen.

**NguyÃªn nhÃ¢n cÃ³ thá»ƒ**:
1. **Forced exploration gÃ¢y nhiá»…u**: Gen 1-2 bá»‹ force random â†’ PPO há»c sai signal
2. **Stale log_probs**: PPO update vá»›i forced actions nhÆ°ng dÃ¹ng log_probs tá»« policy khÃ¡c
3. **Learning rate decay quÃ¡ nhanh**: Gen 20 LR = 0.000038 (gáº§n nhÆ° khÃ´ng há»c)

---

### **3ï¸âƒ£ LGP EVOLUTION DESTABILIZES LEARNING**

**Báº±ng chá»©ng - Best program changes**:
```
Gen | Best | Same as prev?
  1 |  #6  | NO
  2 | #38  | NO  (BEST fitness = -85.96)
  3 | #45  | NO  â† LOST best program!
  4 | #61  | NO
  5 | #14  | NO
...
Best program changed 9/19 times (47.4% instability)
```

**Váº¥n Ä‘á»**: 
- Gen 2 Ä‘áº¡t BEST fitness (-85.96) vá»›i program #38
- Gen 3: Program #38 bá»‹ mutate/replace â†’ fitness drop vá» -136.97
- PPO pháº£i há»c láº¡i tá»« Ä‘áº§u vá»›i program má»›i!

**ğŸ’¡ LGP evolution phÃ¡ hoáº¡i PPO learning!**

---

### **4ï¸âƒ£ POLICY COLLAPSE VáºªN Xáº¢Y RA**

**Báº±ng chá»©ng**:
```
Gen  1: 64/64 programs used (forced)
Gen  5: 6/64 programs used  (9.4%)
Gen 10: 5/64 programs used  (7.8%)
Gen 20: 5/64 programs used  (7.8%)
```

**Váº¥n Ä‘á»**:
- PPO chá»‰ dÃ¹ng 5/64 programs (7.8%)
- Top program chiáº¿m 60% usage
- **KhÃ´ng explore cÃ¡c programs cÃ³ thá»ƒ tá»‘t hÆ¡n!**

**Entropy 0.2 KHÃ”NG Äá»¦ cho action space 64!**

---

### **5ï¸âƒ£ STATE SPACE THIáº¾U THÃ”NG TIN**

**Hiá»‡n táº¡i**:
```python
state = [current_time, num_unfinished, avg_pt]  # Only 3 features!
```

**Váº¥n Ä‘á»**:
- PPO khÃ´ng biáº¿t job nÃ o urgent
- KhÃ´ng biáº¿t slack (thá»i gian dÆ°)
- KhÃ´ng biáº¿t cÃ³ bao nhiÃªu jobs Ä‘ang late
- **â†’ KhÃ´ng Ä‘á»§ thÃ´ng tin Ä‘á»ƒ chá»n action tá»‘i Æ°u!**

---

### **6ï¸âƒ£ REWARD FUNCTION KHÃ”NG INCENTIVIZE CONVERGENCE**

**Hiá»‡n táº¡i**:
```python
reward = -makespan  # Only makespan!
```

**Váº¥n Ä‘á»**:
- Reward = -makespan dao Ä‘á»™ng theo dynamic jobs
- KhÃ´ng cÃ³ bonus cho consistent performance
- KhÃ´ng penalize cho variance cao
- **â†’ PPO khÃ´ng Ä‘Æ°á»£c reward cho viá»‡c STABLE!**

---

## ğŸ“Š SO SÃNH: VÃŒ SAO V2 (10 gen) Há»˜I Tá»¤ Tá»T HÆ N?

| Factor | V2 (old) | V4 (current) | Impact |
|--------|----------|--------------|--------|
| Entropy | 0.01 | 0.2 | V2 exploit nhanh hÆ¡n |
| Forced explore | No | Yes (Gen 1-2) | V4 máº¥t 2 gen khÃ´ng há»c |
| LR decay | No | Yes (0.95^gen) | V4 LR quÃ¡ tháº¥p cuá»‘i |
| Best makespan | 143.16 | 157.72 | V2 tá»‘t hÆ¡n 10% |

**V2 "may máº¯n" converge vÃ o 1 program tá»‘t sá»›m vÃ  exploit 99%!**
**V4 quÃ¡ diverse â†’ khÃ´ng exploit Ä‘Æ°á»£c program tá»‘t nháº¥t!**

---

## ğŸ¯ **GIáº¢I PHÃP TOÃ€N DIá»†N**

### **FIX 1: Reduce Environment Variance (Trung háº¡n)**

```python
# Option A: Fix sá»‘ dynamic jobs
self._generate_dynamic_jobs(num_dynamic=2)  # Always 2, khÃ´ng random

# Option B: Seed-based episodes
def reset(self, seed=None):
    if seed:
        random.seed(seed)
    # ...deterministic dynamic jobs
```

**Expected**: Std giáº£m tá»« 44 â†’ 20-25

---

### **FIX 2: Fix Forced Exploration (Quan trá»ng)**

**Váº¥n Ä‘á» hiá»‡n táº¡i**:
```python
if forced_exploration:
    action = programs_to_explore[forced_idx]  # Random action
    _, log_prob, value = select_action_fn(model, state)  # BUT uses policy's log_prob!
```

**Fix**:
```python
if forced_exploration:
    action = programs_to_explore[forced_idx]
    # DO NOT use this for PPO update!
    # Just collect metrics, don't train
    skip_ppo_update = True
else:
    action, log_prob, value = select_action_fn(model, state)
    skip_ppo_update = False

# Later...
if not skip_ppo_update:
    # PPO update
```

---

### **FIX 3: Hall of Fame - Protect Best Programs (Quan trá»ng)**

```python
# NEVER mutate/replace programs with best-ever fitness
class HallOfFame:
    def __init__(self, size=5):
        self.best_programs = []  # (fitness, program_copy, gen)
    
    def try_add(self, program, fitness, gen):
        if len(self.best_programs) < self.size:
            self.best_programs.append((fitness, deepcopy(program), gen))
            self.best_programs.sort(reverse=True)
        elif fitness > self.best_programs[-1][0]:
            self.best_programs[-1] = (fitness, deepcopy(program), gen)
            self.best_programs.sort(reverse=True)
    
    def get_protected_indices(self, current_library):
        # Return indices that should NEVER be replaced
        ...
```

**Expected**: Gen 2 best (-85.96) sáº½ Ä‘Æ°á»£c maintain!

---

### **FIX 4: Adaptive Entropy Schedule**

```python
def get_entropy(gen, num_gens, base_entropy=0.2):
    """
    High entropy early (explore), low late (exploit)
    """
    progress = gen / num_gens
    if progress < 0.2:
        return 0.4  # High exploration
    elif progress < 0.5:
        return 0.25  # Moderate
    else:
        return 0.15  # Converge
```

---

### **FIX 5: Better Reward for Stability**

```python
# Track rolling average
if not hasattr(self, 'makespan_history'):
    self.makespan_history = []

self.makespan_history.append(makespan)
if len(self.makespan_history) > 10:
    self.makespan_history.pop(0)

# Bonus for beating average
avg_recent = np.mean(self.makespan_history)
if makespan < avg_recent:
    stability_bonus = 10  # Reward for improvement
else:
    stability_bonus = 0

reward = -makespan + stability_bonus
```

---

### **FIX 6: Minimum Learning Rate Floor**

```python
min_lr = 5e-5
current_lr = max(min_lr, initial_lr * (0.95 ** gen))
```

---

## ğŸ“‹ **IMPLEMENTATION PRIORITY**

| Priority | Fix | Impact | Effort |
|----------|-----|--------|--------|
| ğŸ”´ P0 | Hall of Fame (protect best programs) | HIGH | Medium |
| ğŸ”´ P0 | Fix forced exploration (skip PPO update) | HIGH | Low |
| ğŸŸ¡ P1 | Minimum LR floor | Medium | Low |
| ğŸŸ¡ P1 | Adaptive entropy | Medium | Low |
| ğŸŸ¢ P2 | Reduce env variance | Low | Medium |
| ğŸŸ¢ P2 | Better reward | Low | Medium |

---

## ğŸ¯ **Káº¾T LUáº¬N**

**Makespan khÃ´ng há»™i tá»¥ vÃ¬ 3 lÃ½ do chÃ­nh**:

1. **Environment variance CAO** (std ~44): Má»—i episode lÃ  bÃ i toÃ¡n khÃ¡c nhau â†’ KHÃ”NG THá»‚ Ä‘áº¡t makespan stable hoÃ n toÃ n

2. **PPO khÃ´ng há»c Ä‘Æ°á»£c**: Forced exploration gÃ¢y nhiá»…u, LR decay quÃ¡ nhanh, policy tá»‡ Ä‘i trong má»—i generation

3. **LGP evolution phÃ¡ hoáº¡i**: Best program bá»‹ mutate/replace, PPO pháº£i há»c láº¡i tá»« Ä‘áº§u má»—i generation

**Realistic expectation**:
- Vá»›i environment variance hiá»‡n táº¡i: Best possible std ~30-35
- Vá»›i fixes: Makespan cÃ³ thá»ƒ xuá»‘ng 140-150 (thay vÃ¬ 158)
- Perfect convergence (std < 10) lÃ  KHÃ”NG KHáº¢ THI vá»›i dynamic scheduling

---

## âœ… **RECOMMENDED ACTION**

Implement **FIX 1 + FIX 2 + FIX 3** trÆ°á»›c:
1. Hall of Fame Ä‘á»ƒ protect Gen 2 best program (-85.96)
2. Skip PPO update trong forced exploration
3. Minimum LR = 5e-5

**Expected result**:
- Makespan: 145-155 (cáº£i thiá»‡n 5-10%)
- Best fitness maintained: -85.96
- More stable training curve

Báº¡n muá»‘n tÃ´i implement cÃ¡c fixes nÃ y khÃ´ng?
